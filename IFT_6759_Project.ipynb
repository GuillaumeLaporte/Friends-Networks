{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d41facac49f44e90921577696af29566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5efba4b7f5d64e878814b8e851bbdcbe",
              "IPY_MODEL_a8e0e5b57a9a4f51bd35268e315c8021",
              "IPY_MODEL_e48a828c66e44e90a974f6152ead516c"
            ],
            "layout": "IPY_MODEL_b7382d6244324183b7a1656782e04785"
          }
        },
        "5efba4b7f5d64e878814b8e851bbdcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_842a54ba185942899f86d8335a4dda02",
            "placeholder": "​",
            "style": "IPY_MODEL_4e2bbaf38e4f4205ba73c92fdddfcebc",
            "value": "100%"
          }
        },
        "a8e0e5b57a9a4f51bd35268e315c8021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f8ca668d3d402cbaacc01fc91784ee",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a76f322e509c40068bfcdbd2b8c61f1e",
            "value": 170498071
          }
        },
        "e48a828c66e44e90a974f6152ead516c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c5cebb94d9431f91271577cb11abcd",
            "placeholder": "​",
            "style": "IPY_MODEL_541671ff5fcb4cf78a17d17c9733587f",
            "value": " 170498071/170498071 [00:02&lt;00:00, 85615952.53it/s]"
          }
        },
        "b7382d6244324183b7a1656782e04785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "842a54ba185942899f86d8335a4dda02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2bbaf38e4f4205ba73c92fdddfcebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f8ca668d3d402cbaacc01fc91784ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76f322e509c40068bfcdbd2b8c61f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32c5cebb94d9431f91271577cb11abcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541671ff5fcb4cf78a17d17c9733587f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "imageNetModel = torchvision.models.resnet18(pretrained = True)"
      ],
      "metadata": {
        "id": "gDFgCyGaboB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071e8f4e-d782-4086-ac7f-2d08e398595e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputTensor = torch.rand((4,3,224,224))\n",
        "\n",
        "print(\"Original-imageNetModel's output\")\n",
        "print(imageNetModel(inputTensor))\n",
        "\n",
        "print(\"Recreated-imageNetModel's output\")\n",
        "recreatedModel = torch.nn.Sequential(*list(imageNetModel.children())[:-2])\n",
        "print(recreatedModel(inputTensor).size())\n",
        "\n",
        "#print(imageNetModel)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12U79-ETMeG2",
        "outputId": "ab32cbf4-0f89-46ee-8852-51522914122e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original-imageNetModel's output\n",
            "tensor([[-1.2203, -0.9134, -1.2386,  ..., -1.5340,  2.2545,  0.2027],\n",
            "        [-1.1006, -0.0934, -2.3823,  ...,  0.5791,  1.1367,  0.3988],\n",
            "        [ 0.3057,  1.1296,  2.8136,  ...,  0.0373, -1.2903,  0.0877],\n",
            "        [-0.2329, -1.7075, -1.3320,  ..., -1.5365,  2.8195,  3.1354]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Recreated-imageNetModel's output\n",
            "torch.Size([4, 512, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "partialImageNetModel1 = torch.nn.Sequential(*list(imageNetModel.children())[:-5])\n",
        "partialImageNetModel2 = torch.nn.Sequential(*list(imageNetModel.children())[:-4])\n",
        "partialImageNetModel3 = torch.nn.Sequential(*list(imageNetModel.children())[:-3])\n",
        "partialImageNetModel4 = torch.nn.Sequential(*list(imageNetModel.children())[:-2])\n",
        "partialImageNetModels = [partialImageNetModel1, partialImageNetModel2, partialImageNetModel3, partialImageNetModel4]"
      ],
      "metadata": {
        "id": "M7_8KY_nLM4R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(partialImageNetModel2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedBhF7l8v9R",
        "outputId": "258ea23d-9acc-4c73-ad58-14ac9df1b09d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (5): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in partialImageNetModels:\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "v8vvVezMLOPv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below shows how to change a parameter in a convolution layer"
      ],
      "metadata": {
        "id": "LjbOIDDxG3k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convLayer = torch.nn.Conv2d(2,3,4)\n",
        "print(convLayer.state_dict)\n",
        "for (i, param) in enumerate(convLayer.parameters()):\n",
        "  \n",
        "  param.requires_grad = False\n",
        "  print(param)\n",
        "\n",
        "  if i == 0:\n",
        "    print(param[0][0][0][0])\n",
        "    param[0][0][0][0] = 9999\n",
        "    print(param[0][0][0][0])\n",
        "    param.requires_grad = True\n",
        "    print(\"after changing a weight and resetting requires_grad = True\")\n",
        "    print(param)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRS2XEhy5tqC",
        "outputId": "f950cc8e-37e6-46a8-a11b-6eb4c3ca6088"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.state_dict of Conv2d(2, 3, kernel_size=(4, 4), stride=(1, 1))>\n",
            "Parameter containing:\n",
            "tensor([[[[-0.1138, -0.1044,  0.1525, -0.1712],\n",
            "          [ 0.0103,  0.1033, -0.1671,  0.0125],\n",
            "          [ 0.0601,  0.0353,  0.0751, -0.0691],\n",
            "          [-0.1127, -0.0354,  0.1218, -0.0909]],\n",
            "\n",
            "         [[-0.0491,  0.0890, -0.0300,  0.1361],\n",
            "          [-0.0490,  0.0593, -0.1272, -0.0713],\n",
            "          [ 0.0541, -0.0329,  0.1210, -0.0791],\n",
            "          [ 0.1708, -0.0358,  0.1743, -0.0354]]],\n",
            "\n",
            "\n",
            "        [[[-0.0494, -0.0241,  0.0937,  0.0516],\n",
            "          [-0.0335, -0.1217,  0.0063,  0.1393],\n",
            "          [ 0.0134,  0.1295, -0.0998, -0.0077],\n",
            "          [-0.1034,  0.1477,  0.1242, -0.0204]],\n",
            "\n",
            "         [[-0.1014,  0.1084, -0.0279, -0.0648],\n",
            "          [-0.1219,  0.1225, -0.1576,  0.0109],\n",
            "          [-0.1324,  0.0216, -0.1759, -0.0285],\n",
            "          [-0.0882, -0.0245,  0.1535, -0.0441]]],\n",
            "\n",
            "\n",
            "        [[[-0.0106,  0.0719,  0.0983,  0.0281],\n",
            "          [ 0.1503,  0.1462,  0.1660,  0.1162],\n",
            "          [ 0.1702,  0.0198, -0.1088, -0.1572],\n",
            "          [-0.1536,  0.1213,  0.0307,  0.0203]],\n",
            "\n",
            "         [[ 0.1293,  0.1153,  0.1488, -0.0999],\n",
            "          [-0.0426, -0.0849,  0.0675, -0.1278],\n",
            "          [-0.1374,  0.1225, -0.0071, -0.0064],\n",
            "          [-0.0879, -0.1574, -0.0670, -0.0032]]]])\n",
            "tensor(-0.1138)\n",
            "tensor(9999.)\n",
            "after changing a weight and resetting requires_grad = True\n",
            "Parameter containing:\n",
            "tensor([[[[ 9.9990e+03, -1.0438e-01,  1.5248e-01, -1.7122e-01],\n",
            "          [ 1.0313e-02,  1.0326e-01, -1.6714e-01,  1.2460e-02],\n",
            "          [ 6.0133e-02,  3.5349e-02,  7.5137e-02, -6.9136e-02],\n",
            "          [-1.1273e-01, -3.5421e-02,  1.2183e-01, -9.0896e-02]],\n",
            "\n",
            "         [[-4.9144e-02,  8.9035e-02, -2.9998e-02,  1.3607e-01],\n",
            "          [-4.8984e-02,  5.9296e-02, -1.2716e-01, -7.1313e-02],\n",
            "          [ 5.4083e-02, -3.2912e-02,  1.2098e-01, -7.9128e-02],\n",
            "          [ 1.7076e-01, -3.5778e-02,  1.7429e-01, -3.5400e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.9413e-02, -2.4108e-02,  9.3695e-02,  5.1596e-02],\n",
            "          [-3.3471e-02, -1.2171e-01,  6.3310e-03,  1.3931e-01],\n",
            "          [ 1.3447e-02,  1.2955e-01, -9.9803e-02, -7.6866e-03],\n",
            "          [-1.0345e-01,  1.4766e-01,  1.2419e-01, -2.0364e-02]],\n",
            "\n",
            "         [[-1.0137e-01,  1.0836e-01, -2.7943e-02, -6.4772e-02],\n",
            "          [-1.2189e-01,  1.2251e-01, -1.5760e-01,  1.0934e-02],\n",
            "          [-1.3244e-01,  2.1616e-02, -1.7590e-01, -2.8531e-02],\n",
            "          [-8.8204e-02, -2.4496e-02,  1.5349e-01, -4.4082e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0565e-02,  7.1934e-02,  9.8322e-02,  2.8117e-02],\n",
            "          [ 1.5027e-01,  1.4619e-01,  1.6596e-01,  1.1617e-01],\n",
            "          [ 1.7022e-01,  1.9810e-02, -1.0880e-01, -1.5716e-01],\n",
            "          [-1.5359e-01,  1.2125e-01,  3.0670e-02,  2.0265e-02]],\n",
            "\n",
            "         [[ 1.2933e-01,  1.1532e-01,  1.4876e-01, -9.9877e-02],\n",
            "          [-4.2630e-02, -8.4925e-02,  6.7539e-02, -1.2779e-01],\n",
            "          [-1.3745e-01,  1.2251e-01, -7.0926e-03, -6.3674e-03],\n",
            "          [-8.7905e-02, -1.5743e-01, -6.6966e-02, -3.1602e-03]]]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1246, -0.1169,  0.1660])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, we are going to try to create a new convolution layer N1 (that takes as input nC1 channels), from another one N2 (that takes as input nC2 channels); such that N1 copies the parameters from N2; and sets its kernel weights to 0 for (nC1 - nC2) incomming channels. This is the procedure that needs to be implemented when we replace a specific layer in the Simple Network (the one that takes as input the ImageNet network)"
      ],
      "metadata": {
        "id": "rKz864SZHhh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layerToReplace = torch.nn.Conv2d(2,3,4)\n",
        "newLayer = torch.nn.Conv2d(3,3,4)\n",
        "\n",
        "print(\"newLayer biases before change\")\n",
        "print(newLayer.state_dict()[\"bias\"])\n",
        "\n",
        "listOfParameters = []\n",
        "for param in newLayer.parameters():\n",
        "  param.requires_grad = False\n",
        "  listOfParameters.append(param)\n",
        "\n",
        "listOfParameters[0][:,:,:,:] = torch.zeros((3,3,4,4))\n",
        "listOfParameters[0][:,1:,:,:] = layerToReplace.state_dict()[\"weight\"]\n",
        "\n",
        "listOfParameters[1][:] = layerToReplace.state_dict()[\"bias\"]\n",
        "\n",
        "print(\"newLayer biases after change\")\n",
        "print(newLayer.state_dict()[\"bias\"])\n",
        "\n",
        "for param in newLayer.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "print(\"layerToReplace Biases\")\n",
        "print(layerToReplace.state_dict()[\"bias\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goGML8FeHg0T",
        "outputId": "4f603fa8-ba96-43bc-bbef-ce86b46e252d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newLayer biases before change\n",
            "tensor([ 0.0283, -0.0306, -0.0977])\n",
            "newLayer biases after change\n",
            "tensor([ 0.0006, -0.1388, -0.1185])\n",
            "layerToReplace Biases\n",
            "tensor([ 0.0006, -0.1388, -0.1185])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below we do the right concatenation of the output of the lower parts of the ImageNet network and the Simple Network"
      ],
      "metadata": {
        "id": "OzZTNeT7w1Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputFromLowerLayersOfImageNet = torch.rand((1,10,10))\n",
        "outputFromLowerLayersOfSimpleNetwork = torch.rand((2,10,10))\n",
        "\n",
        "#Below is the right order in which to do the concatenation:\n",
        "inputTensorToTheNewLayer = torch.cat([outputFromLowerLayersOfImageNet, outputFromLowerLayersOfSimpleNetwork])\n",
        "\n",
        "#Below is the wrong order in which to do the concatenation:\n",
        "#inputTensorToTheNewLayer = torch.cat([outputFromLowerLayersOfSimpleNetwork, outputFromLowerLayersOfImageNet])\n",
        "\n",
        "print(\"Output from intermediate network before the change\")\n",
        "print(layerToReplace(outputFromLowerLayersOfSimpleNetwork))\n",
        "\n",
        "print(\"Output from intermediate network after the change\")\n",
        "print(newLayer(inputTensorToTheNewLayer))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR7KDDitlsoU",
        "outputId": "0211adb1-3f6d-4523-8733-b43ca71d3afb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from intermediate network before the change\n",
            "tensor([[[-0.1154,  0.2177, -0.1201,  0.2269,  0.0644, -0.1625,  0.2312],\n",
            "         [ 0.1155, -0.2781, -0.2690,  0.0798, -0.2812, -0.1034, -0.1781],\n",
            "         [-0.1159, -0.1781, -0.0622, -0.1370, -0.1824, -0.0233, -0.1408],\n",
            "         [ 0.0865,  0.0282,  0.0849, -0.0512,  0.1036, -0.1275, -0.0855],\n",
            "         [-0.1711,  0.1151, -0.3047,  0.1598, -0.0197, -0.2013,  0.2158],\n",
            "         [-0.0241, -0.0924,  0.0967, -0.0339, -0.0683, -0.1232, -0.4850],\n",
            "         [ 0.0443, -0.1059, -0.0187, -0.1164, -0.3439, -0.0426, -0.2578]],\n",
            "\n",
            "        [[ 0.1554, -0.1286, -0.0681,  0.0878,  0.1427, -0.0650,  0.0046],\n",
            "         [ 0.0493, -0.0502,  0.3071,  0.0939,  0.0907,  0.2917,  0.0873],\n",
            "         [-0.0352,  0.2755,  0.2845, -0.0653,  0.4541,  0.0753,  0.0803],\n",
            "         [ 0.0062,  0.1752, -0.1503,  0.1408,  0.0853,  0.0457,  0.0719],\n",
            "         [-0.1084,  0.0674,  0.1488,  0.2202,  0.0028, -0.0031, -0.0589],\n",
            "         [ 0.2742, -0.0423,  0.0495,  0.1257, -0.0941,  0.0469,  0.0503],\n",
            "         [ 0.2357,  0.0028, -0.1038,  0.1354,  0.2181,  0.0465,  0.1648]],\n",
            "\n",
            "        [[ 0.4234,  0.0850,  0.1385,  0.4280,  0.1453,  0.4703,  0.3018],\n",
            "         [ 0.2980,  0.0012,  0.3120,  0.3239,  0.2863,  0.5122,  0.4817],\n",
            "         [ 0.2714,  0.3371,  0.1451,  0.4903,  0.3079,  0.6048,  0.2555],\n",
            "         [ 0.4609,  0.3739,  0.1350,  0.6346,  0.1627,  0.7401,  0.3029],\n",
            "         [ 0.4625,  0.1427,  0.5035,  0.7602,  0.3544,  0.6009,  0.1040],\n",
            "         [ 0.6085,  0.1086,  0.4678,  0.3954,  0.3136,  0.2710,  0.0209],\n",
            "         [ 0.4396,  0.3936,  0.2699,  0.2703,  0.5267,  0.1054,  0.3037]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Output from intermediate network after the change\n",
            "tensor([[[-0.1154,  0.2177, -0.1201,  0.2269,  0.0644, -0.1625,  0.2312],\n",
            "         [ 0.1155, -0.2781, -0.2690,  0.0798, -0.2812, -0.1034, -0.1781],\n",
            "         [-0.1159, -0.1781, -0.0622, -0.1370, -0.1824, -0.0233, -0.1408],\n",
            "         [ 0.0865,  0.0282,  0.0849, -0.0512,  0.1036, -0.1275, -0.0855],\n",
            "         [-0.1711,  0.1151, -0.3047,  0.1598, -0.0197, -0.2013,  0.2158],\n",
            "         [-0.0241, -0.0924,  0.0967, -0.0339, -0.0683, -0.1232, -0.4850],\n",
            "         [ 0.0443, -0.1059, -0.0187, -0.1164, -0.3439, -0.0426, -0.2578]],\n",
            "\n",
            "        [[ 0.1554, -0.1286, -0.0681,  0.0878,  0.1427, -0.0650,  0.0046],\n",
            "         [ 0.0493, -0.0502,  0.3071,  0.0939,  0.0907,  0.2917,  0.0873],\n",
            "         [-0.0352,  0.2755,  0.2845, -0.0653,  0.4541,  0.0753,  0.0803],\n",
            "         [ 0.0062,  0.1752, -0.1503,  0.1408,  0.0853,  0.0457,  0.0719],\n",
            "         [-0.1084,  0.0674,  0.1488,  0.2202,  0.0028, -0.0031, -0.0589],\n",
            "         [ 0.2742, -0.0423,  0.0495,  0.1257, -0.0941,  0.0469,  0.0503],\n",
            "         [ 0.2357,  0.0028, -0.1038,  0.1354,  0.2181,  0.0465,  0.1648]],\n",
            "\n",
            "        [[ 0.4234,  0.0850,  0.1385,  0.4280,  0.1453,  0.4703,  0.3018],\n",
            "         [ 0.2980,  0.0012,  0.3120,  0.3239,  0.2863,  0.5122,  0.4817],\n",
            "         [ 0.2714,  0.3371,  0.1451,  0.4903,  0.3079,  0.6048,  0.2555],\n",
            "         [ 0.4609,  0.3739,  0.1350,  0.6346,  0.1627,  0.7401,  0.3029],\n",
            "         [ 0.4625,  0.1427,  0.5035,  0.7602,  0.3544,  0.6009,  0.1040],\n",
            "         [ 0.6085,  0.1086,  0.4678,  0.3954,  0.3136,  0.2710,  0.0209],\n",
            "         [ 0.4396,  0.3936,  0.2699,  0.2703,  0.5267,  0.1054,  0.3037]]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def outputSizeOfLowerLayers(lowerLayers, testImage):\n",
        "    return lowerLayers(testImage).size()\n",
        "\n",
        "class MergedNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, lowerLayersOfImageNet, lowerLayersOfSimpleModel, upperLayersOfSimpleModel, layerOfSimpleModelToBeReplaced, specificationsOfLayerToBeReplaced, batchSize):\n",
        "        super(MergedNetwork, self).__init__()\n",
        "        self.lowerLayersOfImageNet = lowerLayersOfImageNet\n",
        "        self.lowerLayersOfSimpleModel = lowerLayersOfSimpleModel\n",
        "        self.upperLayersOfSimpleModel = upperLayersOfSimpleModel\n",
        "\n",
        "        #Cifar10 Images are 32 by 32 with 3 channels (I think (the color channels))\n",
        "        self.outputSizeValuesOfLowerLayersOfSimpleModel = outputSizeOfLowerLayers(lowerLayersOfSimpleModel, torch.zeros((batchSize,3,32,32)))\n",
        "\n",
        "\n",
        "        #the ImageNet network is ResNet18 which takes as input 224 by 244 images with 3 channels\n",
        "        self.outputSizeValuesOfLowerLayersOfImageNetModel = outputSizeOfLowerLayers(lowerLayersOfImageNet, torch.zeros((batchSize,3,224,224)))\n",
        "        \n",
        "        #In the following, the index value of (1) should be the number of channels.\n",
        "        numberOfOutputChannelsOfLowerLayersOfImageNetModel = self.outputSizeValuesOfLowerLayersOfImageNetModel[1]\n",
        "        self.numberOfInputChannelsInNewMergingLayer = self.outputSizeValuesOfLowerLayersOfSimpleModel[1] + numberOfOutputChannelsOfLowerLayersOfImageNetModel\n",
        "\n",
        "        ###########################################################################################\n",
        "        #We construct below the new convolution layer\n",
        "        outNumberOfChannels = specificationsOfLayerToBeReplaced[\"outChannels\"]\n",
        "        kernelSize = specificationsOfLayerToBeReplaced[\"kernelSize\"]\n",
        "        padding = specificationsOfLayerToBeReplaced[\"padding\"]\n",
        "        self.newConvolutionLayer = torch.nn.Conv2d(self.numberOfInputChannelsInNewMergingLayer, outNumberOfChannels, kernelSize, padding = padding)\n",
        "\n",
        "        listOfParameters = []\n",
        "        for param in self.newConvolutionLayer.parameters():\n",
        "          param.requires_grad = False\n",
        "          listOfParameters.append(param)\n",
        "\n",
        "        listOfParameters[0][:,:,:,:] = torch.zeros((outNumberOfChannels, self.numberOfInputChannelsInNewMergingLayer, kernelSize, kernelSize))\n",
        "        listOfParameters[0][:, numberOfOutputChannelsOfLowerLayersOfImageNetModel:,:,:] = layerOfSimpleModelToBeReplaced.state_dict()[\"weight\"]\n",
        "        listOfParameters[1][:] = layerOfSimpleModelToBeReplaced.state_dict()[\"bias\"]\n",
        "\n",
        "        for param in self.newConvolutionLayer.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "        ###########################################################################################\n",
        "        #Becareful here! We don't want to interpolate in the channel dimension!\n",
        "        targetFeatureMapWidth = self.outputSizeValuesOfLowerLayersOfSimpleModel[2]\n",
        "        targetFeatureMapHeight = self.outputSizeValuesOfLowerLayersOfSimpleModel[3]\n",
        "        self.targetShapeOfImageNetOutput = (targetFeatureMapWidth, targetFeatureMapHeight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #The ImageNet model (ResNet18) takes as input 224 by 224 images (with 3 channels).\n",
        "        #Cifar10 Images are 32 by 32 and are color images, so have 3 channels (I think).\n",
        "        inputToImageNet = F.interpolate(x, size = (224, 224))\n",
        "        lowerLayersImageNetOutput = self.lowerLayersOfImageNet(inputToImageNet)\n",
        "\n",
        "        #Becareful here! We don't want to interpolate in the channel dimension!\n",
        "        lowerLayersImageNetOutput = F.interpolate(lowerLayersImageNetOutput, size = self.targetShapeOfImageNetOutput, mode = \"bilinear\")\n",
        "\n",
        "        lowerLayersSimpleModelOutput = self.lowerLayersOfSimpleModel(x)\n",
        "\n",
        "        #Below is the right order in which to do the concatenation:\n",
        "        inputTensorToTheNewLayer = torch.cat([lowerLayersImageNetOutput, lowerLayersSimpleModelOutput], dim = 1)\n",
        "\n",
        "        outputOfNewLayer = self.newConvolutionLayer(inputTensorToTheNewLayer)\n",
        "\n",
        "        return self.upperLayersOfSimpleModel(outputOfNewLayer)"
      ],
      "metadata": {
        "id": "XgkDtBB4Nh1m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "d41facac49f44e90921577696af29566",
            "5efba4b7f5d64e878814b8e851bbdcbe",
            "a8e0e5b57a9a4f51bd35268e315c8021",
            "e48a828c66e44e90a974f6152ead516c",
            "b7382d6244324183b7a1656782e04785",
            "842a54ba185942899f86d8335a4dda02",
            "4e2bbaf38e4f4205ba73c92fdddfcebc",
            "a1f8ca668d3d402cbaacc01fc91784ee",
            "a76f322e509c40068bfcdbd2b8c61f1e",
            "32c5cebb94d9431f91271577cb11abcd",
            "541671ff5fcb4cf78a17d17c9733587f"
          ]
        },
        "id": "f1Y1ou1e0MGn",
        "outputId": "7f2182da-dc52-4747-8387-5ec18588ee47"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d41facac49f44e90921577696af29566"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "XCb6ibZd0QXw",
        "outputId": "aa5f0a79-ab95-4b16-fd60-f1be46a26b5c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog   plane dog   deer \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the model from: https://github.com/Xinyi6/CIFAR10-CNN-by-Keras/blob/master/lic/model2_3.ipynb\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(80))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "I think the \"Dense\" layer in Tensorflow is just a linear layer. The parameter of the \"Dense\" layer is the \"out_features\" parameter of the linear layer in PyTorch. In TensorFlow, you don't need to specify the dimension of the input in a linear layer. You also don't need to specify the number of channels in a convolution layer. In either cases, they are implicitly specified by the previous layer."
      ],
      "metadata": {
        "id": "N7vnYjFWGwST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Below is the __init__ method written such that the .module() method returns the layers in the right order; as to recreate what is spelled out by the forward method\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding = \"same\")\n",
        "        self.relu1 = F.relu\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(p = 0.3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding = \"same\")\n",
        "        self.relu2 = F.relu\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout2 = nn.Dropout(p = 0.3)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding = \"same\")\n",
        "        self.relu3 = F.relu\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout3 = nn.Dropout(p = 0.4)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding = \"same\")\n",
        "        self.relu4 = F.relu\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout4 = nn.Dropout(p = 0.4)\n",
        "\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3, padding = \"same\")\n",
        "        self.relu5 = F.relu\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)\n",
        "        self.dropout5 = nn.Dropout(p = 0.4)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(512, 80)\n",
        "        self.relu6 = F.relu\n",
        "        self.dropout6 = nn.Dropout(p = 0.3)\n",
        "\n",
        "        self.linear2 = nn.Linear(80, 20)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.pool4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.pool5(x)\n",
        "        x = self.dropout5(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.dropout6(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "simpleModel = Net()"
      ],
      "metadata": {
        "id": "-nNBQRgdbjOS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(simpleModel.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "z61NTi9n0aHX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = simpleModel(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ_xMqF70cjQ",
        "outputId": "ec289c1d-c2bb-4af8-eb22-26888f2db3df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 234.679\n",
            "[1,  4000] loss: 225.394\n",
            "[1,  6000] loss: 204.046\n",
            "[1,  8000] loss: 192.756\n",
            "[1, 10000] loss: 186.694\n",
            "[1, 12000] loss: 182.411\n",
            "[2,  2000] loss: 173.501\n",
            "[2,  4000] loss: 169.180\n",
            "[2,  6000] loss: 165.459\n",
            "[2,  8000] loss: 161.327\n",
            "[2, 10000] loss: 154.955\n",
            "[2, 12000] loss: 152.260\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = simpleModel(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXSrzaK-A0M",
        "outputId": "1a616b9e-95b4-44db-ebdf-4fc4bc8505fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 45 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = reconstructedSimpleModel(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yyWh87f-6zS",
        "outputId": "d934070e-139c-4e95-813b-1bc2fd95ae1c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reconstruction of the simpleModel as done below gives the same accuracy as the original model. So the reconstruction is really done right; you really need to add the ReLU activation functions by hand between the Conv2d layers and the Linear layers."
      ],
      "metadata": {
        "id": "mwhhBqLwDBgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "numberOfChildren = len(list(simpleModel.children()))\n",
        "listOfChildren = []\n",
        "for (i, child) in enumerate(simpleModel.children()):\n",
        "  #print(i)\n",
        "  listOfChildren.append(child)\n",
        "  if((isinstance(child, nn.Conv2d) or isinstance(child, nn.Linear)) and (i < numberOfChildren - 1)):\n",
        "    listOfChildren.append(nn.ReLU())\n",
        "print(listOfChildren)\n",
        "reconstructedSimpleModel = torch.nn.Sequential(*listOfChildren)\n",
        "\n",
        "\n",
        "# for (children1, children2) in zip(simpleModel.children(), reconstructedSimpleModel.children()):\n",
        "#   for (param1, param2) in zip(children1.parameters(), children2.parameters()):\n",
        "#     print(torch.all(torch.eq(param1, param2)))\n",
        "\n",
        "# print(\"Some parameters of the last layer of simpleModel\")\n",
        "# for param1 in params1:\n",
        "#   print(param1)\n",
        "# print(\"Some parameters of the last layer of the reconstruction of simpleModel\")\n",
        "# for param2 in params2:\n",
        "#   print(param2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN475HcZ82Mj",
        "outputId": "7724da3e-9adb-4e3a-e895-ecbec44ed771"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.3, inplace=False), Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.3, inplace=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.4, inplace=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.4, inplace=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Dropout(p=0.4, inplace=False), Flatten(start_dim=1, end_dim=-1), Linear(in_features=512, out_features=80, bias=True), ReLU(), Dropout(p=0.3, inplace=False), Linear(in_features=80, out_features=20, bias=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU activation functions get discarded in the reconstruction of the simpleModel network (I think). As the code above shows, all values of the parameters are equal. The ReLU activation functions are not part of the children of the network, so they don't get transfered to the reconstruction of simpleModel. This is why I was getting results that showed values of a different order of magnitude (about may be 10 times), when passing a tensor, to the reconstructed network and the original network."
      ],
      "metadata": {
        "id": "1j07G8UZ2STo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inputTensor = torch.ones((4,3,32,32))\n",
        "\n",
        "print(simpleModel(inputTensor))\n",
        "print(inputTensor.size())\n",
        "print(reconstructedSimpleModel(inputTensor))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eONyO1h8AxLP",
        "outputId": "a38a384d-ac26-46f5-e44f-4a15ba319027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.5820,  0.4644,  3.1070,  0.8278,  2.1320,  0.5174,  0.5390,  0.8805,\n",
            "          4.5138,  1.1418, -2.2141, -1.8552, -1.5743, -1.9809, -2.3326, -2.0321,\n",
            "         -2.1287, -1.9607, -2.1834, -2.2320],\n",
            "        [ 4.8618,  1.6431,  3.6598,  1.2052,  2.0208,  0.1352,  1.0238,  0.2552,\n",
            "          5.6455,  1.9371, -2.3522, -2.3489, -1.9945, -2.8763, -2.1203, -2.5912,\n",
            "         -2.4761, -2.4975, -2.6342, -2.3030],\n",
            "        [ 5.1173,  2.0956,  4.1487,  2.2261,  2.3065,  1.0937,  2.0035,  0.1702,\n",
            "          6.7570,  2.0406, -2.8460, -3.0535, -2.5528, -3.3604, -2.6004, -3.2474,\n",
            "         -3.2761, -3.2111, -3.0593, -2.7875],\n",
            "        [ 5.2180, -0.7999,  4.6917,  2.5948,  3.3590,  2.4468,  1.2543,  1.8520,\n",
            "          4.2601, -0.6171, -2.4455, -2.6407, -2.1782, -2.6511, -2.3869, -2.5555,\n",
            "         -3.1032, -2.8170, -2.7282, -2.3243]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([4, 3, 32, 32])\n",
            "tensor([[ 3.8858,  0.8876,  3.7844,  2.3139,  2.2045,  1.5083,  1.3330,  1.0247,\n",
            "          4.4412,  1.1955, -2.3061, -2.2128, -2.1932, -2.4461, -2.4187, -2.6474,\n",
            "         -2.4903, -2.6022, -2.4424, -2.4517],\n",
            "        [ 5.4574,  0.3144,  4.7604,  2.3765,  3.1603,  1.7859,  1.4991,  1.7139,\n",
            "          5.2878,  0.6804, -2.7898, -2.7072, -2.7128, -3.0266, -2.8957, -2.8765,\n",
            "         -3.2627, -3.2099, -3.0333, -2.6331],\n",
            "        [ 4.3634, -1.0052,  3.6321,  0.6080,  2.1285,  0.1647,  0.8720, -0.1901,\n",
            "          4.7901, -0.7921, -1.4609, -1.9139, -1.0237, -1.8162, -1.5805, -1.6101,\n",
            "         -2.0096, -1.5721, -1.7431, -1.5231],\n",
            "        [ 3.5596, -0.6632,  3.5601,  2.0688,  2.4179,  1.4165,  1.3628,  1.6516,\n",
            "          3.2676, -0.2739, -1.7160, -1.7577, -2.2682, -2.2990, -2.3682, -2.1692,\n",
            "         -2.2472, -2.4695, -2.5366, -1.6824]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we are going to select lower parts and upper parts and the layers to be replaced of the simpleModel network"
      ],
      "metadata": {
        "id": "M6rIyTJ6LEhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfChildren = len(list(simpleModel.children()))\n",
        "\n",
        "partsOfSimpleModellist = []\n",
        "\n",
        "\n",
        "for layerToBeReplacedNumber in range(1,5):\n",
        "\n",
        "  lowerLayers = []\n",
        "  upperLayers = []\n",
        "  \n",
        "  skipChild = False\n",
        "  convolutionLayerNumberOrPlusOne = 0\n",
        "  for (childNumber, child) in enumerate(simpleModel.children()):\n",
        "\n",
        "    if(isinstance(child, nn.Conv2d) and (childNumber > 0)):\n",
        "      convolutionLayerNumberOrPlusOne = convolutionLayerNumberOrPlusOne + 1\n",
        "\n",
        "      if(convolutionLayerNumberOrPlusOne == layerToBeReplacedNumber):\n",
        "        layerToBeReplaced = child\n",
        "        skipChild = True\n",
        "        convolutionLayerNumberOrPlusOne = convolutionLayerNumberOrPlusOne + 1\n",
        "\n",
        "    if(convolutionLayerNumberOrPlusOne > layerToBeReplacedNumber):\n",
        "      \n",
        "      if(skipChild == False):\n",
        "        upperLayers.append(child)\n",
        "\n",
        "      if((isinstance(child, nn.Conv2d) or isinstance(child, nn.Linear)) and (childNumber < numberOfChildren - 1)):\n",
        "        upperLayers.append(nn.ReLU())\n",
        "\n",
        "    elif((convolutionLayerNumberOrPlusOne < layerToBeReplacedNumber) and (skipChild == False)):\n",
        "      lowerLayers.append(child)\n",
        "\n",
        "      if((isinstance(child, nn.Conv2d) or isinstance(child, nn.Linear)) and (childNumber < numberOfChildren - 1)):\n",
        "        lowerLayers.append(nn.ReLU())\n",
        "\n",
        "    skipChild = False\n",
        "    \n",
        "    \n",
        "\n",
        "  partsOfSimpleModellist.append({})\n",
        "  partsOfSimpleModellist[layerToBeReplacedNumber - 1][\"Lower Layers\"] = torch.nn.Sequential(*lowerLayers)\n",
        "  partsOfSimpleModellist[layerToBeReplacedNumber - 1][\"Upper Layers\"] = torch.nn.Sequential(*upperLayers)\n",
        "  partsOfSimpleModellist[layerToBeReplacedNumber - 1][\"Layer to be replaced\"] = layerToBeReplaced\n",
        "\n",
        "print(simpleModel)\n",
        "print(partsOfSimpleModellist[2][\"Lower Layers\"])\n",
        "print(partsOfSimpleModellist[2][\"Upper Layers\"])\n",
        "print(partsOfSimpleModellist[2][\"Layer to be replaced\"])\n",
        "\n",
        "for partsOfSimpleModelIndex in range(4):\n",
        "  for child in partsOfSimpleModellist[partsOfSimpleModelIndex][\"Lower Layers\"]:\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for child in partsOfSimpleModellist[partsOfSimpleModelIndex][\"Upper Layers\"]:\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrlabE-hLDkD",
        "outputId": "a1e4f8fc-1edf-4b21-ab2c-dbbb2628406c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout2): Dropout(p=0.3, inplace=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout3): Dropout(p=0.4, inplace=False)\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout5): Dropout(p=0.4, inplace=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear1): Linear(in_features=512, out_features=80, bias=True)\n",
            "  (dropout6): Dropout(p=0.3, inplace=False)\n",
            "  (linear2): Linear(in_features=80, out_features=20, bias=True)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Dropout(p=0.3, inplace=False)\n",
            "  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (5): ReLU()\n",
            "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Dropout(p=0.3, inplace=False)\n",
            "  (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (9): ReLU()\n",
            "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (11): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "Sequential(\n",
            "  (0): ReLU()\n",
            "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (2): Dropout(p=0.4, inplace=False)\n",
            "  (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Dropout(p=0.4, inplace=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=512, out_features=80, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Dropout(p=0.3, inplace=False)\n",
            "  (11): Linear(in_features=80, out_features=20, bias=True)\n",
            ")\n",
            "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "specificationsOfLayerToBeReplaced = {}\n",
        "specificationsOfLayerToBeReplaced[\"outChannels\"] = 256\n",
        "specificationsOfLayerToBeReplaced[\"kernelSize\"] = 3\n",
        "specificationsOfLayerToBeReplaced[\"padding\"] = \"same\"\n",
        "\n",
        "someMergedNetwork = MergedNetwork(partialImageNetModels[2], partsOfSimpleModellist[2][\"Lower Layers\"], partsOfSimpleModellist[2][\"Upper Layers\"], partsOfSimpleModellist[2][\"Layer to be replaced\"], specificationsOfLayerToBeReplaced, 4)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "l5MBN6TwzoX6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "someMergedNetwork(torch.ones(4, 3, 32, 32))"
      ],
      "metadata": {
        "id": "Y-WxipfHCcu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63864c6-c5d8-4be9-8d18-924c5afedbaa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.6958e+00, -1.0493e+00,  5.3003e+00,  3.7899e+00,  3.6130e+00,\n",
              "          3.4103e+00,  1.5737e+00,  9.8253e-01,  4.4863e+00, -7.6039e-02,\n",
              "         -2.5231e+00, -2.5379e+00, -2.5406e+00, -2.5712e+00, -2.6376e+00,\n",
              "         -2.8253e+00, -2.9916e+00, -2.0888e+00, -2.4733e+00, -2.8049e+00],\n",
              "        [ 4.8533e+00, -4.1061e-01,  5.5880e+00,  4.1840e+00,  3.3369e+00,\n",
              "          4.0043e+00,  1.0619e+00,  1.6187e+00,  4.5576e+00,  1.7279e-01,\n",
              "         -2.5940e+00, -2.7705e+00, -2.7413e+00, -2.8818e+00, -2.3102e+00,\n",
              "         -2.8840e+00, -3.2427e+00, -2.1539e+00, -2.4797e+00, -2.6694e+00],\n",
              "        [ 4.1516e+00,  1.1013e-03,  4.7613e+00,  3.7019e+00,  3.5735e+00,\n",
              "          3.3127e+00,  2.3300e+00,  1.4872e+00,  3.4012e+00,  2.8641e-01,\n",
              "         -3.0192e+00, -2.9382e+00, -2.7155e+00, -2.6649e+00, -3.0450e+00,\n",
              "         -2.8677e+00, -2.9625e+00, -2.4869e+00, -2.2526e+00, -2.7258e+00],\n",
              "        [ 4.6142e+00,  2.2278e-01,  5.1579e+00,  3.2604e+00,  3.5371e+00,\n",
              "          3.1772e+00,  1.6931e+00,  1.3176e+00,  4.9766e+00,  1.3415e+00,\n",
              "         -3.2206e+00, -3.2200e+00, -2.7800e+00, -3.4143e+00, -3.1909e+00,\n",
              "         -2.8482e+00, -3.0934e+00, -2.0998e+00, -2.5168e+00, -2.8721e+00]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleModel(torch.ones(4,3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wd8VKwPQMDx",
        "outputId": "ceef8284-f0a4-43dd-9f71-94ee4562838c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.6148, -0.1527,  5.4538,  4.3333,  3.5902,  3.8770,  1.8116,  1.6738,\n",
              "          4.8480,  0.6237, -3.4249, -3.2900, -3.0724, -3.3043, -3.2290, -3.1719,\n",
              "         -3.6515, -2.5477, -2.6850, -2.9324],\n",
              "        [ 4.1938, -0.5240,  4.6096,  3.0852,  2.6833,  2.6568,  1.2157,  0.4015,\n",
              "          4.5467,  0.1097, -2.2917, -2.2084, -2.3820, -2.3441, -2.4080, -2.3942,\n",
              "         -2.8386, -1.7390, -1.8273, -2.3973],\n",
              "        [ 4.3183, -1.5632,  5.6547,  3.9488,  4.0173,  3.5278,  2.0120,  0.7317,\n",
              "          3.4191, -0.6248, -2.0849, -2.0308, -2.4771, -2.3251, -2.4712, -2.2181,\n",
              "         -2.5568, -1.8424, -1.9825, -2.2541],\n",
              "        [ 4.2617, -0.2148,  5.5542,  4.7587,  3.9762,  4.5425,  2.5437,  2.0424,\n",
              "          3.8375,  0.3250, -3.2171, -3.0290, -2.9726, -3.3927, -3.1424, -3.0821,\n",
              "         -3.3394, -2.3328, -2.5943, -3.0754]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we verify that the accuracy of the merged network is the same as the accuracy of the original network. Because the merged network does not take at this point any input from the lower part of the ImageNet network"
      ],
      "metadata": {
        "id": "5f6rGanHV4sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "index = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = someMergedNetwork(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        if(index % 100 == 99):\n",
        "          print(\"index: \", index)\n",
        "        index = index + 1\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmDqzWnb6O0V",
        "outputId": "58ba4ae3-4dcd-4ec1-d824-976af0cdf5ee"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index:  99\n",
            "index:  199\n",
            "index:  299\n",
            "index:  399\n",
            "index:  499\n",
            "index:  599\n",
            "index:  699\n",
            "index:  799\n",
            "index:  899\n",
            "index:  999\n",
            "index:  1099\n",
            "index:  1199\n",
            "index:  1299\n",
            "index:  1399\n",
            "index:  1499\n",
            "index:  1599\n",
            "index:  1699\n",
            "index:  1799\n",
            "index:  1899\n",
            "index:  1999\n",
            "index:  2099\n",
            "index:  2199\n",
            "index:  2299\n",
            "index:  2399\n",
            "index:  2499\n",
            "Accuracy of the network on the 10000 test images: 44 %\n"
          ]
        }
      ]
    }
  ]
}